{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"<font size=4>\n**Linear Support Vector Classifier (Binary Case)\n**<br>\n<br>\n**The reason why I wrote this article: **explanation of SVC/SVM on the internet is overwhelming, but I could not find anything to clarify all the doubts I have in mind regarding the math part of the Linear Support Vector Classifier.  This article is for people who have the same doubts like me, I hope it will be helpful. If you like it, please click a vote for me, this is my first Kaggle Kernel, thank you! <br>\n<br>\n**Background knowledge required:** Understanding the geometric insights of vector dot product (one vector project to the other vector); Lagrange multiplier (for optimizaiton). <br>\n<br>\n**Notation:  **\n**X** is your sample dataset, it contains N samples, P features. **Xij** is the ith sample, jth feature. **Xi** is the vector describe all the P features of sample i. **X+** is the sample from class \"+\", **X- **is the sample from class \"-\". **Y** is the label vector, **Yi** has 2 values +1 and -1. <br>"},{"metadata":{"_uuid":"7c2bffd43f10b555303ad0568e94409594c530fe"},"cell_type":"markdown","source":"<font size=4>\n**Logic behind Linear SVC: ** <br>\n<br>\nSamples are data points allocate in the P-dimension space, each axis represents one feature. The idea of the Support Vector Classifier is to find the \"hyperplane\" to separate samples into 2 classes in this P-dimension space.  In a 2-D space, when we only have 2 features, SVC is to find the straight line that can separate samples. <br>\n<br>\nOf course, different people will draw different straight lines. However, no matter how you draw the line between these 2 classes, you will always find a closest sample from each class to your straight line. That is the SUPPORT VECTOR. And the distance of these closest samples to your straight line describe how well you separate these two classes, that is the MARGIN. (Since there too many tutorials of SVM, all the figures in this article are from the web. )<br>\n![](http://www.saedsayad.com/images/SVM_2.png)<br>\nSo, how to draw this line? <br>\nFirst, we need to make the straight line \"unbiased\" to any class. So the distance from + support vector to the line should equal to the - support vector; <br> \nSecond, if the margin value is small, it means it will be too sensitive to these support vectors. If you change dataset, the support vector will vary and your classifier will not be robust. As the figure below, all the lines are biased or \"support vector sensitive\". <br>\n![](http://docs.opencv.org/2.4/_images/separating-lines.png)<br>\n**The goal of the Linear SVC is to MAXIMIZE the width of the margin. ** <br>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"<font size=4>\n**Describe the problem in a mathmetical way first. **<br>\n<br>\n$\\bullet$ The straight line in the simple case above can be expressed as: <br>\n$β_{0} + β_{0}*x_{1} + β_{1}*x_{2}=0$. <br>\nLinear algebra friendly, I'd like to think it as , <br>\n$ β_{0}*x_{1} + β_{1}*x_{2}=- β_{0}$, that is the dot product of 2 vectors: $\\vec w = [ β_{0},  β_{1}]$, and $\\vec x = [x_{1}, x_{2}]$. <br>\n$\\vec w$ is the vector that is perpendicular to the straight line, and $\\vec x$ is the point on the straight line. <br>\nAnd of course, here is a more general way to quantify the \"hyperplane\" we are interested to find: <br>\n$ β_{0}+β_{0}*x_{1} + β_{1}*x_{2}..+β_{p}*x_{p}=0$ <br>\nor $\\vec w \\bullet \\vec x + b =0$ .<br>\n<br>\n$\\bullet$ Once we have the \"hyperplane\", we can define our DECISION RULE. <br>\nWe'd like for \"+\" and \"-\" class sample, they will lie on different side of the \"hyperplane\". <br>\nFor support vectors, to describe it in mathmatial way is $\\vec w \\bullet \\vec X_{+} + b = +1 $ and $\\vec w \\bullet \\vec X_{-} + b = -1$. <br>\nFor other samples, is $\\vec w \\bullet \\vec X_{+} + b > +1 $ and $\\vec w \\bullet \\vec X_{-} + b < -1$. <br>\nPut label vector **Y** to make these two functions into one for mathmatical convinience $Y_{i}(\\vec w \\bullet \\vec X_{i} + b ) >=1$. <br>\n<br>\n$\\bullet$ Describe MARGIN in a mathmatical way. <br>\nPair of support vectors, $X_{+}-X_{-}$ projects onto $\\vec w$ is the width of the margin. (If you understand the geometric insights of vector dot product, this will be so easy to understand).<br>\n![](http://i.stack.imgur.com/IMXfV.png)<br>\nSo, $\\frac{(\\vec X_{+} - \\vec X_{-}) \\bullet \\vec w}{||w||}$ is the width of the margin. <br>\nSince $Y_{i}(\\vec w \\bullet \\vec X_{i} + b ) =1$ is true, <br>\n$\\vec X_{+} \\vec w=1-b $, and $\\vec X_{-} \\vec w = -1-b$, <br>\nWe can rewrite $\\frac{(\\vec X_{+} - \\vec X_{-}) \\bullet \\vec w}{||w||}$ into $\\frac{2}{||w||}$. <br>\nAnd our goal is to MAXIMIZE $\\frac{2}{||w||}$. <br>\nAgain, for mathmatical convenience, we can just get the MINIMIZE of $\\frac{1}{2} ||w||^2$. <br>"},{"metadata":{"_uuid":"8891439d9777bb0da411e161cae48266fe07718f"},"cell_type":"markdown","source":"<font size=4>\n**Optimize our goal function. <br>\n**\n$\\frac{1}{2} ||w||^2$ function is our target function, but it comes with a constrained function $Y_{i}(\\vec w \\bullet \\vec X_{i} + b ) =1$. <br>\nLAGRANGIAN TRANSFORM is a tool to find the optimization of a target function with constrains. <br>\n3blue1brown gives a wonderful insight into this tool, I'll paste the link here if you are interested.\nhttps://www.youtube.com/watch?v=hQ4UNu1P2kw <br>\nNow, let's pack them together. <br>\nL = $\\frac{1}{2} ||w||^2$ - $\\sum \\alpha _{i} [Y_{i}(\\vec w \\bullet \\vec X_{i} + b ) -1] $ <br>\n$\\frac{\\partial L}{\\partial w}=0$, we will have $\\vec w = \\sum \\alpha _{i} Y_{i} X_{i}$. <br>\n$\\frac{\\partial L}{\\partial b}=0$, we will have $\\sum \\alpha _{i} Y_{i} =0$. <br>\nPlug these 2 euqations back to L, <br>\nL = $\\frac{1}{2}(\\sum \\alpha _{i} Y_{i} \\vec X_{i})(\\sum \\alpha _{j} Y_{j} \\vec X_{j}) - \\sum \\alpha_{i}[Y_{i}(\\sum \\alpha _{j} Y_{j} \\vec X_{i} \\bullet \\vec X_{j} + b) - 1]$ = $-\\frac{1}{2} \\sum _{ij} \\alpha _{i} \\alpha _{j} Y_{i} Y_{j} \\vec X_{i} \\bullet \\vec X_{j} + \\sum \\alpha _{i}$. <br>\nFrom this we can see, the classifier is only depends on the dot product of the sample. <br>"},{"metadata":{"_uuid":"ba2dab5b194ac12432092531cd7bc0e93932435d"},"cell_type":"markdown","source":"<font size=4>\n**Algorithm for computer to find the solution for the L equation above. <br>\n**\nSMO is one of the most popular algorithms there. This article only focuses on the math part, so I will not talk about it here. <br>"},{"metadata":{"_uuid":"1eac3706b420cc9fd3dc134f36273bec1e8ebb24"},"cell_type":"markdown","source":"<font size=4>\n**Soft margin SVC <br>\n**\nWhen you use the python Linear SVC function, there are 2 parameters you can control. One is the alpha, which is the same alpha we talked about it before.  The larger alpha is, the wider your margin is. <br>\nThe other parameter is C, it comes from \"Soft margin SVC\". <br>\nSince most datasets are not so ideal to separate perfectly by a straight line, we can allow a few samples to go across it, which we call soft margin. Like this, <br>\n![](http://www.researchgate.net/profile/Catarina_Moreira2/publication/260283043/figure/fig12/AS:297261608259590@1447884098130/Figure-A14-Soft-margin-linear-SVM-classifier.png) <br>\nThen we need to rewrite our model into, <br>\n$Y_{i}(\\vec w \\bullet \\vec X_{i} + b ) =1 - C_{i}$. <br>\nThe larger the C is, the more mislabeled sample you allowed. It will increase the robustness of your model, but it will decrease your precision. <br>"},{"metadata":{"_uuid":"aabb5e5024288dce629d50149d6b5d19832f8a00"},"cell_type":"markdown","source":"<font size=4>\n**About the Kernels <br>\n**\nAnother import part of SVC is the kernel. It allows user to project the P-dimension dataset into other dimensions for a better seperation. <br>\nA kernel is a tranform function. It takes in P-dimension dataset and spits out the transformation new dataset. <br>\nOne simple example is, a 2D linear non-seperatable dataset, after transforming it into 3D, it becomes seperable.  <br>\n![](http://www.eric-kim.net/eric-kim-net/posts/1/imgs/data_2d_to_3d_hyperplane.png) <br>"},{"metadata":{"_uuid":"cb0c9fc31d1d69ec4049dc68e994f2f51c23d005"},"cell_type":"markdown","source":"<font size=4>\n**About the Normalization **<br>\n<br>\nAnother thing to keep in mind is, before you feed the SVC model, please make sure you've scaled your data or normalized it. Cause it will affect the result a lot. \n<br>\n**That will be all for the Linear Support Vector Classifier. If you like it, please vote for me. Thank you! ** <br>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}